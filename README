# JAX-based Deep Learning Framework

This repository contains implementations of deep learning models and training pipelines using JAX and Equinox.

## Project Structure

- `CIFAR10_Classification.py`: Main script for training and evaluating models on CIFAR-10
- `models.py`: Contains model architectures (MLP, MLPClassifier)
- `trainer_module.py`: Training utilities, data loaders, and logging functionality
- `loss_fn.py`: Loss function implementations (MSE, Cross-Entropy)
- `optimizer.py`: Optimizer configurations
- `regression.py`: Example script for regression tasks

## Setup

1. Create a conda environment:
```bash
conda create -n UvA python=3.13
conda activate UvA
```

2. Install dependencies:
```bash
pip install jax jaxlib equinox optax torch torchvision tqdm matplotlib tensorboard
```

3. Create a data directory:
```bash
mkdir -p ../data
```

## Usage

### CIFAR-10 Classification

Train a model on CIFAR-10:
```bash
python CIFAR10_Classification.py --batch_size 256 --learning_rate 1e-3 --num_epochs 100 --use_tensorboard
```

Optional arguments:
- `--seed`: Random seed (default: 42)
- `--batch_size`: Batch size (default: 256)
- `--num_workers`: Number of data loading workers (default: 4)
- `--learning_rate`: Learning rate (default: 1e-3)
- `--num_epochs`: Number of training epochs (default: 100)
- `--use_tensorboard`: Enable TensorBoard logging

### Regression

Train a model on regression tasks:
```bash
python regression.py
```

## Features

- JAX/Equinox-based model implementations
- PyTorch data loading pipeline
- TensorBoard logging support
- Model checkpointing
- Batch normalization and dropout support
- Cross-entropy and MSE loss functions
- Adam optimizer with learning rate scheduling

I would like to acknowledge the original tutorials and authors.

@misc{lippe2024uvadlc,
   title        = {{UvA Deep Learning Tutorials}},
   author       = {Phillip Lippe},
   year         = 2024,
   howpublished = {\url{https://uvadlc-notebooks.readthedocs.io/en/latest/}}
}