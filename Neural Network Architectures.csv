Architecture Type,"Origin (Who, When, Where)",Learning Type,Key Characteristics,Problem/Applications,Tasks & Subtasks (SOTA Performance),Advantages,Limitations/Drawbacks,Current Research Directions / Scope for Improvement
Multilayer Perceptron (MLP),"F. Rosenblatt (Perceptron, 1957); Backprop (various, 1980s)",Supervised,"Feedforward, fully connected layers, non-linear activation functions.","Tabular data classification/regression, simple pattern recognition.",Tabular: Competitive for highly curated data.,"Simple, general non-linear function approximator.",Not suited for high-dimensional or structured data; no inductive bias.,As building blocks; efficient training for large MLPs.
Convolutional Neural Network (CNN),"LeNet-5 (Yann LeCun et al., 1998, Bell Labs); AlexNet (Krizhevsky et al., 2012, University of Toronto)","Supervised, Self-supervised","Convolutional layers (shared weights, local receptive fields), pooling layers, hierarchical feature extraction.","Computer Vision (image/video), some audio.","Image Classification (ImageNet: ResNet, EfficientNet, ConvNeXt), Object Detection (YOLO, Faster R-CNN, DETR - for backbones), Semantic/Instance Segmentation (U-Net, Mask R-CNN).",Excellent for spatial data; translational invariance; parameter sharing.,Struggles with long-range dependencies; lacks global context; limited rotational/scale invariance.,Combining with attention/Transformers; more efficient architectures; robust to adversarial attacks.
Recurrent Neural Network (RNN),"David Rumelhart, 1986","Supervised, RL, Self-supervised","Recurrent connections, hidden state for memory.","Sequential data (text, speech, time series).","Machine Translation, Speech Recognition, Text Generation (historically; now largely superseded by Transformers).",Handles variable-length sequences; models temporal dependencies.,Vanishing/exploding gradients; difficulty with very long dependencies; slow parallelization.,Specialized use cases where Transformers are too complex/expensive; recurrent mechanisms within Transformers.
Long Short-Term Memory (LSTM),"Hochreiter & Schmidhuber, 1997, Technical University of Munich","Supervised, RL, Self-supervised","Gated memory cells (input, forget, output gates) to control information flow.","Sequential data (text, speech, time series).",Improved versions of RNN tasks; particularly effective for longer sequences.,Mitigates vanishing/exploding gradients; learns longer dependencies than vanilla RNNs.,Still computationally expensive for very long sequences; slow parallelization.,Continued use in niche areas; baseline for sequence modeling.
Gated Recurrent Unit (GRU),"Cho et al., 2014, University of Montreal","Supervised, RL, Self-supervised","Simplified LSTM with fewer gates (reset, update).","Sequential data (text, speech, time series).","Similar to LSTMs, often competitive performance.",Simpler than LSTMs; faster to train; effective.,Similar limitations to LSTMs regarding very long sequences and parallelization.,"Similar to LSTMs, often a choice when less complexity is desired."
Transformer Network,"Vaswani et al., 2017, Google Brain","Self-supervised (pre-training), Supervised (fine-tuning)","Self-attention mechanism, positional encodings, feedforward layers.","NLP, Computer Vision, Multimodal.","NLP: Machine Translation, Text Generation (GPT-3/4, Llama), QA (BERT, T5), Summarization. CV: Image Classification (ViT, Swin Transformer), Object Detection (DETR). Multimodal: Text-to-Image (DALL-E, Stable Diffusion), VQA.",Captures long-range dependencies; highly parallelizable; scales well with data and parameters; strong transfer learning.,Quadratic complexity for self-attention; data hungry; lacks inherent spatial/sequential inductive bias.,Efficiency (sparse/linear attention); multimodality; long-context understanding; reasoning & factuality; interpretability; safety.
Generative Adversarial Network (GAN),"Goodfellow et al., 2014, University of Montreal",Unsupervised (for learning data distributions),Generator and Discriminator networks engaged in a zero-sum game.,"Image/video generation, image-to-image translation, data augmentation.","Realistic Image Synthesis (StyleGAN, BigGAN), Image Style Transfer (CycleGAN).",Generates highly realistic and diverse data; learns complex distributions; unsupervised.,Training instability (mode collapse); difficult evaluation; sensitive to hyperparameters.,Improving training stability; mitigating mode collapse; better evaluation metrics; conditional GANs; combination with Diffusion Models.
Variational Autoencoder (VAE),"Kingma & Welling, 2013, University of Amsterdam","Unsupervised, Self-supervised","Encoder maps to probabilistic latent space (mean, variance), Decoder samples from latent space to reconstruct input.","Dimensionality reduction, feature learning, anomaly detection, generative modeling.",Anomaly detection; representation learning for various downstream tasks.,Probabilistic framework; smooth latent space for interpolation; unsupervised.,Generated samples can be blurry; posterior collapse; less sharp than GANs.,More expressive decoders; addressing posterior collapse; combining with discrete latent spaces (VQ-VAE); conditional VAEs.
Deep Q-Network (DQN),"DeepMind, 2013-2015",Reinforcement Learning,"CNN/MLP approximates Q-value function; uses experience replay, target networks.","Game playing (discrete action spaces), simple control tasks.","Atari Games (Rainbow DQN, SOTA on many classic games).",Learns directly from raw high-dimensional inputs; stabilizes Q-learning.,Sample inefficient; prone to overestimation of Q-values; limited to discrete action spaces.,Improved exploration; combining with policy-based methods; addressing instability.
"Actor-Critic (PPO, SAC)","Actor-Critic (1990s); PPO (Schulman et al., 2017, OpenAI); SAC (Haarnoja et al., 2018, UC Berkeley)",Reinforcement Learning,Two networks: Actor (policy) and Critic (value function).,"Robotics, continuous control, complex decision-making.",Robotic Manipulation; Continuous Control (SOTA for many continuous action tasks).,Addresses continuous action spaces; often more stable than pure policy gradient or value-based methods.,Sample inefficiency; hyperparameter sensitivity; complex reward engineering.,Sample efficiency; generalization to new environments; offline RL; multi-agent RL; explainable RL.
Graph Neural Network (GNN),"Scarselli et al., 2008; GCN (Kipf & Welling, 2017, University of Amsterdam)","Supervised, Unsupervised, Self-supervised",Processes graph-structured data by message passing/aggregation.,"Social networks, drug discovery, material science, knowledge graphs, point clouds.",Node/Graph Classification/Regression; Link Prediction; Molecular Property Prediction (SOTA in many chemo/bioinformatics tasks).,Handles irregular graph data; learns complex relationships; permutation invariance.,Scalability for large graphs; over-smoothing in deep networks; handling heterogeneous/dynamic graphs.,Scalability; deeper and more expressive architectures; interpretability; hybrid models with Transformers.
Physics-Informed Neural Network (PINN),"Raissi et al., 2017, Brown University",Supervised (with physics regularization),Incorporates physical laws (PDEs) into the loss function.,"Solving differential equations, inverse problems, material science, fluid dynamics.","Solving certain PDEs (e.g., Navier-Stokes) with limited data; parameter discovery.","Leverages physics knowledge, reduces data reliance, solves forward & inverse problems.",Can be computationally expensive for complex PDEs; convergence guarantees; selection of hyperparameters.,Robustness & convergence; extending to complex systems; combining with data-driven methods; uncertainty quantification.
U-Net,"Ronneberger et al., 2015, University of Freiburg",Supervised,Symmetric encoder-decoder with skip connections for precise localization.,Medical Image Segmentation.,"Medical Image Segmentation (SOTA on many datasets like cell segmentation, organ segmentation in CT/MRI).",Excellent for segmentation tasks; leverages both low-level and high-level features; works well with limited data.,Specific to segmentation; can be sensitive to image quality; generalization across different medical centers.,3D U-Nets; attention mechanisms in U-Net; self-supervised pre-training for medical images.